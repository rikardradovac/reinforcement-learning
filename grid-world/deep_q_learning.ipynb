{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning for Grid World Navigation\n",
    "\n",
    "This notebook explores using deep Q-learning to solve a grid world navigation problem with dynamic obstacles. While traditional Q-learning with explicit state-action value tables works well for small state spaces, many real-world problems have state spaces that are too large to enumerate completely. Here we'll see how neural networks can be used to approximate the Q-function in these cases.\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "We'll work with a 10x10 grid world where an agent needs to navigate to a goal while avoiding fire that can spread randomly after each move. The state space is massive - with 100 possible agent positions and each cell either containing fire or not, there are 100 * 2^100 â‰ˆ 10^32 possible states. This makes it impractical to store explicit Q-values for every state-action pair.\n",
    "\n",
    "Instead, we'll use a neural network to approximate the Q-function, taking the state as input and outputting action values for moving up, down, left or right. This allows us to capture the underlying patterns in optimal behavior without having to store values for every possible state.\n",
    "\n",
    "### Key Implementation Challenges\n",
    "\n",
    "There are two main challenges when using neural networks for Q-learning:\n",
    "\n",
    "1. **Catastrophic Forgetting**: Neural networks can \"forget\" previously learned behaviors when trained on new experiences. For example, if the network learns that moving left is good when the goal is on the left, it may overwrite this knowledge when later trained on a situation where moving left leads to failure. This is addressed using experience replay - storing past experiences in a memory buffer and training on random batches from this buffer.\n",
    "\n",
    "2. **Training Instability**: Since the network is learning from its own predictions of future rewards, training can become unstable and diverge. This is mitigated by using separate policy and target networks:\n",
    "   - The policy network determines actions and is updated regularly\n",
    "   - The target network estimates future rewards and is updated less frequently by copying weights from the policy network\n",
    "\n",
    "### Background\n",
    "\n",
    "This approach builds on the seminal paper \"Human-level control through deep reinforcement learning\" (Nature, 2015) which demonstrated deep Q-learning's effectiveness on Atari games. The key insight was that neural networks could learn meaningful state representations and action-value functions directly from high-dimensional sensory inputs.\n",
    "\n",
    "### State Representation \n",
    "\n",
    "The state is represented as three 10x10 grids (channels):\n",
    "- Channel 1: Agent position (1 at agent location, 0 elsewhere)\n",
    "- Channel 2: Fire locations (1 where fire exists, 0 elsewhere) \n",
    "- Channel 3: Goal position (fixed)\n",
    "\n",
    "This representation allows the neural network to learn spatial patterns and relationships between the agent, obstacles, and goal.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### State representation\n",
    "\n",
    "To represent the state we have used an array consisting of three $10\\times 10$ grid layers. The first specifies the position of the player with a 1 at the position of the player, otherwise zero, and if the player has walked outside the grid all elements are zero. The second layer specifies the position of the fire, with 1 where there is a fire and 0 otherwise. The third layer represents the goal and is fixed at the same position. I.e. <code>state[:,:,0]</code> gives the grid describing the position of the player, <code>state[:,:,1]</code> position of the fire and <code>state[:,:,2]</code> position of the goal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Libraries for plotting\n",
    "import matplotlib  \n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits import mplot3d\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# For safe copy of varaibles\n",
    "import copy\n",
    "\n",
    "# Nice way of building a memory, a list with maximum size\n",
    "from collections import deque\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GridWorld:\n",
    "     \n",
    "    def __init__(self):\n",
    "        # the size of the grid\n",
    "        self.size = np.array([10,10])\n",
    "        # number of layers in state \n",
    "        self.layers = 3\n",
    "        \n",
    "        # Default starting position and goal\n",
    "        self.start = np.array([1,8])\n",
    "        self.goalpos = np.array([8,1])      \n",
    "        \n",
    "        # rewards, gravel refers to an ordinary step \n",
    "        self.cliff = -100\n",
    "        self.fire = -50\n",
    "        self.goal = 100\n",
    "        self.gravel = -1\n",
    "                \n",
    "        # Default values for network\n",
    "        self.gamma = 0       \n",
    "        # Probability of wind\n",
    "        self.wind = 0\n",
    "        #probability for fire to spread\n",
    "        self.prob_spread = 0\n",
    "        \n",
    "        # Default values for epsilon greedy, not optimal, updated further down!\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.99999999999999999999999999\n",
    "        self.epsilon_min = 0.1\n",
    "        \n",
    "        # Memory, default values, not optimal, updated further down!\n",
    "        self.memory_size = 1 \n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        self.batchSize = 1 \n",
    "        \n",
    "    # Constructs the state, random_placement is either True or False. If True the player is placed \n",
    "    # randomly, if False the player is initialized in the starting position\n",
    "    \n",
    "    def make_state(self, random_placement: bool):\n",
    "        \n",
    "        if random_placement:\n",
    "            r_x = np.random.randint(self.size[0]) \n",
    "            r_y = np.random.randint(self.size[1])\n",
    "            # if random = goal keep generating values \n",
    "            while r_x == self.goalpos[0] and r_y == self.goalpos[1]:\n",
    "                r_x = np.random.randint(self.size[0]) \n",
    "                r_y = np.random.randint(self.size[1])\n",
    "        else :\n",
    "            r_x = self.start[0]\n",
    "            r_y = self.start[1]\n",
    "        \n",
    "        # Initialize all values in all layers to zero\n",
    "        state = np.zeros((self.size[0],self.size[1],self.layers))\n",
    "        # we will use player_coordinate to keep track of the position of the player\n",
    "        player_coordinate = [0,0]\n",
    "\n",
    "        # Go through all layers and put 1 at the correct position\n",
    "        for x in range(self.size[0]) : \n",
    "            for y in range(self.size[1]) :\n",
    "            \n",
    "                # Player, first layer\n",
    "                if x == r_x and y == r_y :\n",
    "                    state[x,y,0] = 1\n",
    "                    player_coordinate[0] = x\n",
    "                    player_coordinate[1] = y\n",
    "                else : \n",
    "                    state[x,y,0] = 0\n",
    "\n",
    "                       \n",
    "                # Fire, second layer\n",
    "                if (1<= x <=2) and (1<= y <= 2):\n",
    "                    state[x,y,1] = 1    \n",
    "                else :\n",
    "                    state[x,y,1] = 0 \n",
    "            \n",
    "                #Goal, thrid layer\n",
    "                if x == self.goalpos[0] and y == self.goalpos[1] :\n",
    "                    state[x,y,2] = 1\n",
    "                else : \n",
    "                    state[x,y,2] = 0                \n",
    "    \n",
    "        # return state and player_coordinate\n",
    "        return state , player_coordinate \n",
    "\n",
    "    \n",
    "    def make_move(self, state: np.ndarray, action: int, player_coordinate: np.ndarray, is_wind: bool):\n",
    "        # Use deepcopy to make a copy of the state, otherwise this would just be a pointer to the same\n",
    "        \n",
    "        next_state = copy.deepcopy(state)\n",
    "                \n",
    "        if is_wind :\n",
    "            if np.random.rand() < self.wind:\n",
    "                # overwrite action with random action internally\n",
    "                action = np.random.randint(4)\n",
    "                \n",
    "        new_x = player_coordinate[0]\n",
    "        new_y = player_coordinate[1]\n",
    "        \n",
    "        # Assume that the player goes out of the board ,set old position to zero\n",
    "        # and new coordinate to none\n",
    "        next_state[new_x,new_y,0] = 0\n",
    "        next_player_coordinate = None\n",
    "        done = True\n",
    "        reward = self.cliff\n",
    "        \n",
    "        # make move \n",
    "        if action < 2: # up or down\n",
    "            if action == 0: # up\n",
    "                new_y = new_y + 1\n",
    "            else : # down\n",
    "                new_y = new_y - 1 \n",
    "        else : # left or right\n",
    "            if action == 2: # left \n",
    "                new_x = new_x - 1\n",
    "            else : # right\n",
    "                new_x = new_x + 1   \n",
    "\n",
    "        # If inside grid \n",
    "        if 0 <= new_x < self.size[0] and 0 <= new_y < self.size[1]:\n",
    "            # if it hits the goal\n",
    "            if state[new_x,new_y,2] == 1:\n",
    "                done = True\n",
    "                reward = self.goal\n",
    "                next_state[new_x,new_y,0] = 1\n",
    "                next_player_coordinate = [new_x,new_y]\n",
    "            # fire # WHAT IF THE GOAL BURNS?\n",
    "            elif state[new_x,new_y,1] == 1:\n",
    "                done = False\n",
    "                reward = self.fire\n",
    "                next_state[new_x,new_y,0] = 1\n",
    "                next_player_coordinate = [new_x,new_y]\n",
    "            # gravel    \n",
    "            else : \n",
    "                done = False \n",
    "                reward = self.gravel\n",
    "                next_state[new_x,new_y,0] = 1\n",
    "                next_player_coordinate = [new_x,new_y]\n",
    "            \n",
    "        # else, do nothing, next_player coordinate remains None and next_state[:,:,0] remains all zeros\n",
    "            \n",
    "        return next_state, next_player_coordinate , reward , done  \n",
    "\n",
    "    \n",
    "    def let_fire_spread(self, state: np.ndarray):\n",
    "        new_state = copy.deepcopy(state)\n",
    "        \n",
    "        # Assume fire did not spread\n",
    "        did_fire_spread = False\n",
    "    \n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                # Walk trhough the fire-grid, if encountering fire, see if it spreads\n",
    "                if state[x,y,1] == 1 : \n",
    "                    # with the probability self.prob_spread the fire spreads \n",
    "                    if np.random.rand() < self.prob_spread :\n",
    "                        # a random move , no diagonal moves are allowed\n",
    "                        if np.random.rand() < 0.5 :\n",
    "                            x_step = np.random.randint(2)*2-1\n",
    "                            y_step = 0\n",
    "                        else :\n",
    "                            x_step = 0\n",
    "                            y_step = np.random.randint(2)*2-1\n",
    "                        # if within boundaries\n",
    "                        if (0<= x+x_step < self.size[0]) and (0<= y+y_step < self.size[1]):\n",
    "                            # Check that this square is not allready on fire\n",
    "                            if new_state[x+x_step,y+y_step,1] == 0: \n",
    "                                # LET IT BURN!\n",
    "                                new_state[x+x_step,y+y_step,1] = 1\n",
    "                                did_fire_spread = True\n",
    "                                \n",
    "        \n",
    "        return new_state , did_fire_spread\n",
    "    \n",
    "    # This function implements training and experience replay with two approaches. The currently active approach, version 1,\n",
    "    # utilizes the original predictions for target values. The alternative approach, version 2, which is currently commented out,\n",
    "    # employs the target network to generate new target values for past states. Our findings suggest that version 1 is more stable\n",
    "    # for this specific problem, but both versions can be experimented with.\n",
    "    def replay(self, policy_model):\n",
    "        if len(self.memory) < self.batchSize:\n",
    "            return  # Skip training if not enough samples\n",
    "            \n",
    "        minibatch = random.sample(self.memory, self.batchSize)\n",
    "        \n",
    "        state_batch = torch.FloatTensor(np.array([m[0] for m in minibatch]))\n",
    "        target_batch = torch.FloatTensor(np.array([m[1] for m in minibatch]))\n",
    "\n",
    "        # Prioritize goal states in training\n",
    "        for _ in range(5):  # Multiple passes over each batch\n",
    "            policy_model.optimizer.zero_grad()\n",
    "            \n",
    "            current_q_values = policy_model(state_batch)\n",
    "            target_q_values = target_batch.clone()\n",
    "            \n",
    "            # Update targets with more emphasis on goal states\n",
    "            for i, (_, _, action, reward, _, next_q_max, done) in enumerate(minibatch):\n",
    "                if done and reward == self.goal:  # Goal state\n",
    "                    target_q_values[i][action] = reward * 1.2  # Boost goal state values\n",
    "                elif done:  # Failed state\n",
    "                    target_q_values[i][action] = reward\n",
    "                else:  # Normal state\n",
    "                    target_q_values[i][action] = reward + self.gamma * next_q_max\n",
    "            \n",
    "            loss = policy_model.loss_fn(current_q_values, target_q_values)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Add gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            policy_model.optimizer.step()\n",
    "            \n",
    "    # Used to display the grid\n",
    "    def make_RGB_grid(self,state,path):\n",
    "        grid_RGB = np.ones((self.size[0],self.size[1],3))*0.7 #\n",
    "        \n",
    "        if path is not None :\n",
    "            for i,location in enumerate(path):\n",
    "                grid_RGB[location[0],location[1],:] = np.array([0,0,0]) # black'P' #player\n",
    "    \n",
    "        for x in range(self.size[0]) : \n",
    "            for y in range(self.size[1]) :\n",
    "            \n",
    "                if state[x,y,2]==1:\n",
    "                    grid_RGB[x,y,:] = np.array([245/255,237/255,48/255]) # Yellow\n",
    "                \n",
    "                if state[x,y,1]==1:\n",
    "                    grid_RGB[x,y,:] = np.array([203/255,32/255,40/255]) # Red '-' #pit    \n",
    "   \n",
    "                if state[x,y,0]==1:\n",
    "                    grid_RGB[x,y,:] = np.array([0/255,254/255,0/255]) # Green '-' #pit    \n",
    "   \n",
    "        return grid_RGB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, world):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # Conv layers\n",
    "        self.conv1 = nn.Conv2d(world.layers, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.batch_norm = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        # After first conv (10x10) -> pool (5x5) -> second conv (5x5)\n",
    "        self.flatten_size = 32 * 5 * 5\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.005)  #\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ensure input is in correct format (batch_size, channels, height, width)\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, channels, height, width)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        x = F.relu(self.conv1(x))   # (batch, 32, 10, 10)\n",
    "        x = self.pool(x)            # (batch, 32, 5, 5)\n",
    "        x = F.relu(self.conv2(x))   # (batch, 32, 5, 5)\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        x = x.reshape(-1, self.flatten_size)  # Changed from view to reshape\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def setup_network(world):\n",
    "    model = QNetwork(world)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The network architecture is designed to process visual inputs effectively. It begins with a convolutional layer that transforms the input into a feature map of size (8, 8, 32). This is followed by a max pooling layer, which reduces the spatial dimensions to (4, 4, 32) while retaining the most important features. This process not only extracts key features but also reduces the number of parameters, making the network more efficient. The output is then fed into another convolutional layer, resulting in a feature map of size (2, 2, 32). A batch normalization layer is applied to stabilize the training process by normalizing the activations. The output is then flattened into a 1-dimensional vector of size 128, preparing it for the dense layers. The first dense layer has 64 output values, allowing the network to learn more complex representations. Finally, the output is fed into a second dense layer with 4 output values, suitable for the task at hand.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup network and GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network and Target Network in Deep Q-Learning\n",
    "\n",
    "Deep Q-Learning uses two neural networks - a policy network and a target network - to improve training stability and performance.\n",
    "\n",
    "#### Policy Network\n",
    "The policy network is the primary network that:\n",
    "- Actively learns from experiences\n",
    "- Determines which actions to take during gameplay\n",
    "- Gets updated frequently through experience replay\n",
    "- Receives gradient updates based on the loss between predicted and target Q-values\n",
    "\n",
    "#### Target Network\n",
    "The target network is a periodic copy of the policy network that:\n",
    "- Updates less frequently (every N episodes)\n",
    "- Provides stable target Q-values for training\n",
    "- Helps prevent oscillations and divergence in training\n",
    "\n",
    "#### Why Two Networks?\n",
    "Using two networks addresses a fundamental challenge in Q-learning called the \"moving targets\" problem:\n",
    "\n",
    "1. If we used a single network, we would be trying to predict Q-values while simultaneously using those same predictions as training targets. This creates an unstable feedback loop.\n",
    "\n",
    "2. By using a separate target network that updates less frequently, we create more stable target values. The policy network can learn against these fixed targets, making the training process more reliable.\n",
    "\n",
    "3. This approach helps break the correlation between predictions and targets, reducing harmful feedback loops that could destabilize training.\n",
    "\n",
    "In our implementation:\n",
    "- The policy network is updated every 10 games through experience replay\n",
    "- The target network is updated every `update_target_network_period` games by copying weights from the policy network\n",
    "- The target network provides the Q-value estimates used in calculating training targets, while the policy network determines actual gameplay actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=800, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (loss_fn): MSELoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup GridWorld and Q-networks\n",
    "\n",
    "# Create world from GridWorld\n",
    "world = GridWorld()\n",
    "# Setup network\n",
    "policy_model = setup_network(world)\n",
    "#make a target network as well\n",
    "target_model = setup_network(world)\n",
    "# copy weights from policy to target\n",
    "target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "# Print model architecture\n",
    "print(policy_model)\n",
    "\n",
    "#Make state \n",
    "state , player_coordinate = world.make_state(False)\n",
    "\n",
    "# plot it \n",
    "grid_RGB =world.make_RGB_grid(state,None)\n",
    "#\n",
    "fig=plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# We have to invert the x and y axis , go over to numpy array instead\n",
    "plt.imshow(np.swapaxes(np.array(grid_RGB),0,1))\n",
    "#plt.axis('on')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(np.arange(0, world.size[0], dtype=int))\n",
    "plt.yticks(np.arange(0, world.size[1], dtype=int))\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network on model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE YOU NEED TO DEFINE PARAMETER VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup system parameters\n",
    "\n",
    "world.gamma = 0.99\n",
    "world.epsilon_decay = 0.99 #multiplicative factor that reduces epsilon each step, for no reduction use 1\n",
    "world.epsilon = 0.9  #initial value of epsilon \n",
    "world.wind = 0.1\n",
    "# fire spreading\n",
    "world.prob_spread = 0.3 # \n",
    "\n",
    "#update the target network every \"update_target_network_period game\". Updating target network less often should make\n",
    "#the system more stable, but also convergence slower\n",
    "update_target_network_period = 20\n",
    "\n",
    "#define size of experience replay buffer (how many moves are stored for training) \n",
    "#and batchsize (how many moves from memory buffer are used in each training instance)\n",
    "world.memory_size = 10000\n",
    "        \n",
    "world.batchSize = 128\n",
    "world.memory = deque(maxlen=world.memory_size)   #The experience replay memory\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnostics. These are used to store the max and min, q-values output by the network for the states visited since the last time the target network was updated. If the training has converged these should correspond quite well to the maximal and minimal rewards available in the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training diagnostics.  \n",
    "q_max = 0\n",
    "q_min = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop. It should be all set to run if you have defined the network and parameters above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 2999\n",
      "Epsilon :  0.0996\n",
      "Step count : 7\n",
      "End pos [8, 1]\n",
      "Since updated target: Qmin  -49.91869 Qmax 110.73793\n"
     ]
    }
   ],
   "source": [
    "# MAIN LOOP with network\n",
    "step_count = 0\n",
    "random_start = True  # Easier to train from random start\n",
    "is_wind = False  # Set to false to simplify training \n",
    "next_player_coordinate = None\n",
    "nr_games = 3000  \n",
    "\n",
    "# Set networks to training mode\n",
    "policy_model.train()\n",
    "target_model.eval()  # Target network should be in eval mode\n",
    "\n",
    "for games in range(nr_games):\n",
    "    # Display\n",
    "    print(\"Game #: %s\" % (games,))\n",
    "    print(\"Epsilon : %7.4f\" % world.epsilon)     \n",
    "    print(\"Step count : %s\" % step_count) \n",
    "    print(\"End pos %s\" % next_player_coordinate)\n",
    "    print(\"Since updated target: Qmin  %s Qmax %s\" % (q_min,q_max))\n",
    "    \n",
    "    # Initialize new game\n",
    "    state, player_coordinate = world.make_state(random_start)\n",
    "    step_count = 0\n",
    "\n",
    "    while True:\n",
    "        step_count += 1\n",
    "        \n",
    "        # Convert state to tensor and get Q-values\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_state = policy_model(state_tensor).squeeze(0).detach().numpy()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        action = np.argmax(q_state) if np.random.rand() > world.epsilon else np.random.randint(4)\n",
    "        \n",
    "        # Make move\n",
    "        next_state, next_player_coordinate, reward, done = world.make_move(state, action, player_coordinate, is_wind)\n",
    "        \n",
    "        # Get next state max Q-value using target network\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            next_q_max = target_model(next_state_tensor).max().item()\n",
    "        \n",
    "        # Store experience\n",
    "        world.memory.append((state, q_state, action, reward, next_state, next_q_max, done))\n",
    "        \n",
    "        # Update diagnostics\n",
    "        q_max = max(q_max, np.max(q_state))\n",
    "        q_min = min(q_min, np.min(q_state))\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if done or step_count > 400:\n",
    "            break\n",
    "            \n",
    "        # Update state and handle fire spread\n",
    "        state = next_state\n",
    "        player_coordinate = next_player_coordinate\n",
    "        new_state, fire_spread = world.let_fire_spread(state)\n",
    "        if fire_spread:\n",
    "            state = new_state\n",
    "    \n",
    "    # Training phase (every 10 games)\n",
    "    if games % 10 == 0 and games > 0:\n",
    "        world.replay(policy_model)\n",
    "        \n",
    "        # Epsilon decay\n",
    "        if world.epsilon > world.epsilon_min:\n",
    "            world.epsilon *= world.epsilon_decay\n",
    "    \n",
    "    # Update target network\n",
    "    if games % update_target_network_period == 0:\n",
    "        print(\"Update target network\")\n",
    "        target_model.load_state_dict(policy_model.state_dict())\n",
    "        q_max = 0\n",
    "        q_min = 0\n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting : State value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# State value function of network, i.e. max over actions of the Q-function\n",
    "\n",
    "# Initialize state, this means that the fire is in the starting position\n",
    "state, player_coordinate = world.make_state(False)\n",
    "\n",
    "# set position to zero\n",
    "state[player_coordinate[0],player_coordinate[1],0] = 0\n",
    "\n",
    "# to plot\n",
    "z = np.zeros((world.size[0],world.size[1]))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "policy_model.eval()\n",
    "\n",
    "# Go through all possible position of the player and calculate the value of the best action\n",
    "# according to the network\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for x in range(world.size[0]):\n",
    "        for y in range(world.size[1]):\n",
    "            player_coordinate = [x,y]\n",
    "            state[player_coordinate[0],player_coordinate[1],0] = 1\n",
    "            \n",
    "            # Convert state to tensor\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Get Q-values and convert to numpy\n",
    "            q_state = policy_model(state_tensor).squeeze(0).numpy()\n",
    "            z[x,y] = q_state.max()\n",
    "            \n",
    "            # Reset state\n",
    "            state[x,y,0] = 0\n",
    "\n",
    "# Set model back to training mode\n",
    "policy_model.train()\n",
    "\n",
    "# Plot        \n",
    "plt.figure()\n",
    "fig = plt.figure(figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.imshow(np.swapaxes(z,0,1))\n",
    "plt.colorbar()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(np.arange(0, world.size[0], dtype=int))\n",
    "plt.yticks(np.arange(0, world.size[1], dtype=int))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting : Dynamic play\n",
    "\n",
    "To challenge the agent increase the probability of fire spread, and turn on and increase the wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=800, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (loss_fn): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dynamic game replay\n",
    "\n",
    "for a in range(1):\n",
    "    # fire spreading\n",
    "    world.prob_spread = 0.6\n",
    "    world.wind = 0.1\n",
    "    is_wind = True\n",
    "\n",
    "    # get original state  \n",
    "    state, player_coordinate = world.make_state(True)\n",
    "    path = np.array([player_coordinate])\n",
    "    \n",
    "    # setup figure\n",
    "    fig = plt.figure(figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    policy_model.eval()\n",
    "\n",
    "    done = False\n",
    "    count = 0\n",
    "    while (not done) and (count < 40):\n",
    "        count = count + 1\n",
    "        \n",
    "        # plot it \n",
    "        plot_grid = world.make_RGB_grid(state, path)\n",
    "        plt.imshow(np.swapaxes(np.array(plot_grid), 0, 1))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.xticks(np.arange(0, world.size[0], dtype=int))\n",
    "        plt.yticks(np.arange(0, world.size[1], dtype=int))\n",
    "        # clear figure and wait\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "        # find action using PyTorch\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_state = policy_model(state_tensor).squeeze(0).numpy()\n",
    "            \n",
    "        # get best action, no epsilon greedy\n",
    "        action = np.argmax(q_state)      \n",
    "        \n",
    "        # make the move\n",
    "        next_state, next_player_coordinate, reward, done = world.make_move(state, action, player_coordinate, is_wind)\n",
    "\n",
    "        # update state \n",
    "        state = next_state\n",
    "        player_coordinate = next_player_coordinate\n",
    "\n",
    "        if not done:\n",
    "            path = np.append(path, [player_coordinate], axis=0)\n",
    "\n",
    "        # let fire spread\n",
    "        new_state, fire_spread = world.let_fire_spread(state)\n",
    "        if fire_spread:\n",
    "            state = new_state \n",
    "        \n",
    "        if int(state[:, :, 1][player_coordinate[0], player_coordinate[1]]) == 1:\n",
    "            break\n",
    "        \n",
    "\n",
    "    plt.close()\n",
    "\n",
    "# Set model back to training mode\n",
    "policy_model.train()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
